# -*- coding: utf-8 -*-
"""ex_spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TJvYzNmPhHd9qBJMKcxQLw9FMibiFYP5
"""

!pip install pyspark

"""# Importando as bibliotecas necessárias e configurando o Spark:"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, expr
import random

spark = SparkSession \
    .builder \
    .master ("local[*]") \
    .appName("Exercicio Intro") \
    .getOrCreate()

print("Spark configurado com sucesso!")

"""# Etapa 1: Ler o arquivo e mostrar o DataFrame"""

df_nomes = spark.read.text("nomes_aleatorios.txt").toDF("nomes")

df_nomes.show(5)

"""# Etapa 2: Renomear a coluna e verificar o schema"""

df_nomes = df_nomes.withColumnRenamed("nomes", "Nomes")

df_nomes.printSchema()

df_nomes.show(10)

"""# Etapa 3: Adicionar coluna "Escolaridade"
"""

escolaridade_opcoes = ["Fundamental", "Medio", "Superior"]

df_nomes = df_nomes.withColumn(
    "Escolaridade",
    expr(f"array('{escolaridade_opcoes[0]}', '{escolaridade_opcoes[1]}', '{escolaridade_opcoes[2]}')[cast(rand() * 3 as int)]")
)

df_nomes.show(10)

"""# Etapa 4: Adicionar coluna "País"
"""

paises = ["Argentina", "Bolívia", "Brasil", "Chile", "Colômbia", "Equador",
          "Guiana", "Paraguai", "Peru", "Suriname", "Uruguai", "Venezuela", "Guiana Francesa"]

# expressão para atribuir um país aleatório
paises_expr = "array(" + ", ".join([f"'{pais}'" for pais in paises]) + ")[cast(rand() * 13 as int)]"

# adicionando a coluna "Pais" ao DataFrame
df_nomes = df_nomes.withColumn("Pais", expr(paises_expr))

df_nomes.show(10)

"""# Etapa 5: Adicionar coluna "AnoNascimento"
"""

# valores aleatórios entre 1945 e 2010
df_nomes = df_nomes.withColumn(
    "AnoNascimento",
    expr("cast(rand() * (2010 - 1945 + 1) + 1945 as int)")
)

df_nomes.show(10)

"""# Etapa 6: Selecionar pessoas nascidas neste século"""

df_select = df_nomes.filter(col("AnoNascimento") >= 2000)

df_select.select("Nomes").show(10)

"""# Etapa 7: Repetir com Spark SQL"""

df_nomes.createOrReplaceTempView("pessoas")

df_sql_select = spark.sql("SELECT Nomes FROM pessoas WHERE AnoNascimento >= 2000")
df_sql_select.show(10)

"""# Etapa 8: Contar Millenials (1980-1994)"""

millennials_count = df_nomes.filter(
    (col("AnoNascimento") >= 1980) & (col("AnoNascimento") <= 1994)
).count()

print(f"Número de Millenials: {millennials_count}")

"""# Etapa 9: Contar Millenials com Spark SQL"""

millennials_sql = spark.sql(
    "SELECT COUNT(*) AS Millenials FROM pessoas WHERE AnoNascimento BETWEEN 1980 AND 1994"
)
millennials_sql.show()

"""# Etapa 10: Contar pessoas por país e geração"""

# criando a coluna "Geração"
df_nomes = df_nomes.withColumn(
    "Geracao",
    expr("""
        CASE
            WHEN AnoNascimento BETWEEN 1944 AND 1964 THEN 'Baby Boomers'
            WHEN AnoNascimento BETWEEN 1965 AND 1979 THEN 'Geração X'
            WHEN AnoNascimento BETWEEN 1980 AND 1994 THEN 'Millennials'
            WHEN AnoNascimento BETWEEN 1995 AND 2015 THEN 'Geração Z'
        END
    """)
)

# tabela temporária
df_nomes.createOrReplaceTempView("pessoas")

# contando pessoas por país e geração
query = """
    SELECT Pais, Geracao, COUNT(*) AS Quantidade
    FROM pessoas
    GROUP BY Pais, Geracao
    ORDER BY Pais, Geracao, Quantidade
"""
df_resultado = spark.sql(query)
df_resultado.show()